{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import countDistinct, row_number\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "dbutils.widgets.text(\"idunidade_operacional\", \"3\")\n",
    "dbutils.widgets.text(\"idcidade\", \"3\")\n",
    "dbutils.widgets.text(\"periodo_inicial\", '2017-01-01')\n",
    "dbutils.widgets.text(\"periodo_final\", '2019-03-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">6</span><span class=\"ansired\">]: </span>DataFrame[key: string, value: string]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sqlContext.sql(\"SET periodo_inicial = \" + dbutils.widgets.get(\"periodo_inicial\"))\n",
    "sqlContext.sql(\"SET periodo_final = \" + dbutils.widgets.get(\"periodo_final\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">7</span><span class=\"ansired\">]: </span>DataFrame[key: string, value: string]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unidade_operacional = dbutils.widgets.get(\"idunidade_operacional\")\n",
    "cidade = dbutils.widgets.get(\"idcidade\")\n",
    "nome_tabela_p = \"\"\n",
    "nome_tabela_cad = \"\"\n",
    "nome_tabela_cortes = \"\"\n",
    "\n",
    "if (unidade_operacional == \"17\"):\n",
    "  if (cidade == \"43\"):\n",
    "    nome_tabela_pcr = \"modelos_preditivos.kpi_saneatins_palmas_processos_\"\n",
    "    nome_tabela_cortes = \"modelos_preditivos.kpi_saneatins_palmas_cortes_\"\n",
    "    \n",
    "  elif (cidade != \"43\"):\n",
    "    nome_tabela_pcr = \"modelos_preditivos.kpi_saneatins_all_processos_\"\n",
    "    nome_tabela_cortes = \"modelos_preditivos.kpi_saneatins_all_cortes_\"\n",
    "    \n",
    "elif (unidade_operacional == \"3\"):\n",
    "  if (cidade == \"3\"):\n",
    "    nome_tabela_p = \"modelos_preditivos.kpi_maua_maua_processos_\"\n",
    "    nome_tabela_cortes = \"modelos_preditivos.kpi_maua_maua_cortes_\"\n",
    "    nome_tabela_cad = \"modelos_preditivos.kpi_maua_maua_cadastros_\"\n",
    "  \n",
    "sqlContext.sql(\"SET nome_tabela_p = \" + nome_tabela_p)\n",
    "sqlContext.sql(\"SET nome_tabela_cortes = \" + nome_tabela_cortes)\n",
    "sqlContext.sql(\"SET nome_tabela_cad = \" + nome_tabela_cad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_cortes_periodo():\n",
    "  \n",
    "  sql_cortes_periodo = \"\"\"\n",
    "                        select year(periodoabertura) as ano_corte, month(periodoabertura) as mes_corte, count(distinct idligacao) as cortados\n",
    "                        from ${hiveconf:nome_tabela_cortes}\n",
    "                        group by periodoabertura\n",
    "                        order by periodoabertura              \n",
    "                       \"\"\"\n",
    "  df_cortes_periodo = sqlContext.sql(sql_cortes_periodo)\n",
    "  return df_cortes_periodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.catalyst.parser.ParseException: \n",
       "mismatched input ')' expecting {'PRECEDING', 'FOLLOWING'}(line 5, pos 172)\n",
       "\n",
       "== SQL ==\n",
       "select\n",
       " *,\n",
       " year(data_corte) ano_corte,\n",
       " month(data_corte) mes_corte,\n",
       " count(distinct data_corte) over (partition by cdc_proc_lig, idclienteinquilino order by data_corte range between dateadd(current_date, - 1) and dateadd(current_date, -360)) as qtde_cortes12m_previos\n",
       "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^\n",
       "from modelos_preditivos.kpi_maua_maua_processos_\n",
       "where idobjetivoprocessocorte = 2\n",
       "and not statusligacaoprocesso = 'C'\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n",
       "\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n",
       "\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:53)\n",
       "\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n",
       "\tat com.databricks.sql.parser.DatabricksSqlParser$$anonfun$parsePlan$1.apply(DatabricksSqlParser.scala:54)\n",
       "\tat com.databricks.sql.parser.DatabricksSqlParser$$anonfun$parsePlan$1.apply(DatabricksSqlParser.scala:51)\n",
       "\tat com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:74)\n",
       "\tat com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:51)\n",
       "\tat org.apache.spark.sql.SparkSession$$anonfun$6.apply(SparkSession.scala:694)\n",
       "\tat org.apache.spark.sql.SparkSession$$anonfun$6.apply(SparkSession.scala:694)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:693)\n",
       "\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:707)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:87)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:33)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:381)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat scala.collection.immutable.List.map(List.scala:285)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:33)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:136)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:324)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:304)\n",
       "\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:45)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:45)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:304)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)\n",
       "\tat scala.util.Try$.apply(Try.scala:192)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:475)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:542)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:381)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:328)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:122)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:136)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:324)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:304)\n",
       "\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:45)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:45)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:304)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)\n",
       "\tat scala.util.Try$.apply(Try.scala:192)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:475)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:542)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:381)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:328)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)\n",
       "\tat java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select\n",
    " *,\n",
    " year(data_corte) ano_corte,\n",
    " month(data_corte) mes_corte,\n",
    " count(distinct data_corte) over (partition by cdc_proc_lig, idclienteinquilino order by data_corte range between dateadd(current_date, - 1) and dateadd(current_date, -360)) as qtde_cortes12m_previos\n",
    "from modelos_preditivos.kpi_maua_maua_processos_\n",
    "where idobjetivoprocessocorte = 2\n",
    "and not statusligacaoprocesso = 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: org.apache.spark.sql.catalyst.parser.ParseException: \n",
       "mismatched input 'from' expecting <EOF>(line 2, pos 0)\n",
       "\n",
       "== SQL ==\n",
       "select t3.ano_corte, t3.mes_corte, t4.cortados, t3.cortados_nao_c_12mprev\n",
       "from (\n",
       "^^^\n",
       "select t2.ano_corte, t2.mes_corte, count (distinct t2.cortado_naocortado12mprevios) cortados_nao_c_12mprev\n",
       "from (select t1.*,\n",
       "      case \n",
       "        when (t1.qtde_cortes12m_previos <1) and (not t1.data_corte is null ) \n",
       "        then t1.cdc_proc_lig else null end as cortado_naocortado12mprevios\n",
       "      from (\n",
       "             select\n",
       "                 *,\n",
       "                 year(data_corte) ano_corte,\n",
       "                 month(data_corte) mes_corte,\n",
       "                 count(distinct data_corte) over (partition by cdc_proc_lig, idclienteinquilino order by data_corte range between dateadd(current_date, - 1) and dateadd(current_date, -360)) as qtde_cortes12m_previos\n",
       "             from modelos_preditivos.kpi_maua_maua_processos_\n",
       "             where idobjetivoprocessocorte = 2\n",
       "             and not statusligacaoprocesso = 'C') t1\n",
       "        )t2\n",
       "       group by t2.ano_corte, t2.mes_corte\n",
       "    ) t3\n",
       "inner join cortes_por_periodo t4\n",
       "on t3.ano_corte = t4.ano_corte\n",
       "and t3.mes_corte = t4.mes_corte\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n",
       "\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n",
       "\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:53)\n",
       "\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n",
       "\tat com.databricks.sql.parser.DatabricksSqlParser$$anonfun$parsePlan$1.apply(DatabricksSqlParser.scala:54)\n",
       "\tat com.databricks.sql.parser.DatabricksSqlParser$$anonfun$parsePlan$1.apply(DatabricksSqlParser.scala:51)\n",
       "\tat com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:74)\n",
       "\tat com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:51)\n",
       "\tat org.apache.spark.sql.SparkSession$$anonfun$6.apply(SparkSession.scala:694)\n",
       "\tat org.apache.spark.sql.SparkSession$$anonfun$6.apply(SparkSession.scala:694)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:693)\n",
       "\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:707)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:87)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:33)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:381)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat scala.collection.immutable.List.map(List.scala:285)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:33)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:136)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:324)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:304)\n",
       "\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:45)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:45)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:304)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)\n",
       "\tat scala.util.Try$.apply(Try.scala:192)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:475)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:542)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:381)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:328)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:122)\n",
       "\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:136)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:324)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$7.apply(DriverLocal.scala:304)\n",
       "\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:45)\n",
       "\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:45)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:304)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589)\n",
       "\tat scala.util.Try$.apply(Try.scala:192)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:475)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:542)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:381)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:328)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215)\n",
       "\tat java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select t3.ano_corte, t3.mes_corte, t4.cortados, t3.cortados_nao_c_12mprev\n",
    "from (\n",
    "      select t2.ano_corte, t2.mes_corte, count(distinct t2.cortado_naocortado12mprevios) cortados_nao_c_12mprev\n",
    "      from (select t1.*,\n",
    "            case \n",
    "              when (t1.qtde_cortes12m_previos <1) and (not t1.data_corte is null) \n",
    "              then t1.cdc_proc_lig else null end as cortado_naocortado12mprevios\n",
    "            from (\n",
    "                   select\n",
    "                       *,\n",
    "                       year(data_corte) ano_corte,\n",
    "                       month(data_corte) mes_corte,\n",
    "                       count(distinct data_corte) over (partition by cdc_proc_lig, idclienteinquilino order by data_corte range between dateadd(current_date, - 1) and dateadd(current_date, -360)) as qtde_cortes12m_previos\n",
    "                   from modelos_preditivos.kpi_maua_maua_processos_\n",
    "                   where idobjetivoprocessocorte = 2\n",
    "                   and not statusligacaoprocesso = 'C') t1\n",
    "              )t2\n",
    "         group by t2.ano_corte, t2.mes_corte\n",
    "         ) t3\n",
    "inner join cortes_por_periodo t4\n",
    "on t3.ano_corte = t4.ano_corte\n",
    "and t3.mes_corte = t4.mes_corte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_cortados_naocortados12mesesprev():\n",
    "  df_cortes_periodo = get_cortes_periodo()\n",
    "  df_cortes_periodo.createOrReplaceTempView('cortes_por_periodo')\n",
    "  \n",
    "  sql_cortados_naocortados12M = \"\"\"\n",
    "                                select t3.ano_corte, t3.mes_corte, t4.cortados, t3.cortados_nao_c_12mprev\n",
    "                                from (\n",
    "                                      select t2.ano_corte, t2.mes_corte, count (distinct cortado_naocortado12mprevios) cortados_nao_c_12mprev\n",
    "                                      from (\n",
    "                                            select\n",
    "                                            t1.*,\n",
    "                                            case \n",
    "                                              when (t1.qtde_cortes12m_previos <1) and (not t1.data_corte is null ) \n",
    "                                              then cdc_proc_lig else null end as cortado_naocortado12mprevios\n",
    "                                            from (\n",
    "                                                  select\n",
    "                                                  *,\n",
    "                                                  year(data_corte) ano_corte,\n",
    "                                                  month(data_corte) mes_corte,\n",
    "                                                  count(distinct data_corte) over (partition by cdc_proc_lig, idclienteinquilino order by data_corte range between dateadd(current_date, - 1) and dateadd(current_date, -360)) as qtde_cortes12m_previos\n",
    "                                                  from ${hiveconf:nome_tabela_p}\n",
    "                                                  where idobjetivoprocessocorte = 2\n",
    "                                                  and not statusligacaoprocesso = 'C') t1\n",
    "                                            )t2\n",
    "                                      group by t2.ano_corte, t2.mes_corte\n",
    "                                    ) t3\n",
    "                                  inner join cortes_por_periodo t4\n",
    "                                  on t3.ano_corte = t4.ano_corte\n",
    "                                  and t3.mes_corte = t4.mes_corte\n",
    "                                \"\"\"\n",
    "  \n",
    "  df_cortados_12m_previos = sqlContext.sql(sql_cortados_naocortados12M)\n",
    "  return df_cortados_12m_previos\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansired\">ParseException</span>                            Traceback (most recent call last)\n",
       "<span class=\"ansigreen\">&lt;command-3161241364107805&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n",
       "<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>df_cortados_12m_previos  <span class=\"ansiyellow\">=</span> get_cortados_naocortados12mesesprev<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">&lt;command-3161241364107803&gt;</span> in <span class=\"ansicyan\">get_cortados_naocortados12mesesprev</span><span class=\"ansiblue\">()</span>\n",
       "<span class=\"ansigreen\">     30</span>                                 &quot;&quot;&quot;\n",
       "<span class=\"ansigreen\">     31</span> <span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">---&gt; 32</span><span class=\"ansiyellow\">   </span>df_cortados_12m_previos <span class=\"ansiyellow\">=</span> sqlContext<span class=\"ansiyellow\">.</span>sql<span class=\"ansiyellow\">(</span>sql_cortados_naocortados12M<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">     33</span>   <span class=\"ansigreen\">return</span> df_cortados_12m_previos<span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">     34</span> <span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/context.py</span> in <span class=\"ansicyan\">sql</span><span class=\"ansiblue\">(self, sqlQuery)</span>\n",
       "<span class=\"ansigreen\">    356</span>         <span class=\"ansiyellow\">[</span>Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row1&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row2&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row3&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    357</span>         &quot;&quot;&quot;\n",
       "<span class=\"ansigreen\">--&gt; 358</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>sparkSession<span class=\"ansiyellow\">.</span>sql<span class=\"ansiyellow\">(</span>sqlQuery<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    359</span> <span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    360</span>     <span class=\"ansiyellow\">@</span>since<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1.0</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">sql</span><span class=\"ansiblue\">(self, sqlQuery)</span>\n",
       "<span class=\"ansigreen\">    827</span>         <span class=\"ansiyellow\">[</span>Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row1&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row2&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>f1<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">,</span> f2<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;row3&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    828</span>         &quot;&quot;&quot;\n",
       "<span class=\"ansigreen\">--&gt; 829</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> DataFrame<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jsparkSession<span class=\"ansiyellow\">.</span>sql<span class=\"ansiyellow\">(</span>sqlQuery<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_wrapped<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    830</span> <span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    831</span>     <span class=\"ansiyellow\">@</span>since<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">2.0</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n",
       "<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n",
       "<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n",
       "</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n",
       "<span class=\"ansigreen\">     71</span>                 <span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">     72</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.catalyst.parser.ParseException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">---&gt; 73</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">raise</span> ParseException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">     74</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.streaming.StreamingQueryException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">     75</span>                 <span class=\"ansigreen\">raise</span> StreamingQueryException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansired\">ParseException</span>: &quot;\\nmismatched input &apos;from&apos; expecting &lt;EOF&gt;(line 3, pos 32)\\n\\n== SQL ==\\n\\n                                select t3.ano_corte, t3.mes_corte, t4.cortados, t3.cortados_nao_c_12mprev\\n                                from (\\n--------------------------------^^^\\n                                      select t2.ano_corte, t2.mes_corte, count (distinct cortado_naocortado12mprevios) cortados_nao_c_12mprev\\n                                      from (\\n                                            select\\n                                            t1.*,\\n                                            case \\n                                              when (t1.qtde_cortes12m_previos &lt;1) and (not t1.data_corte is null ) \\n                                              then cdc_proc_lig else null end as cortado_naocortado12mprevios\\n                                            from (\\n                                                  select\\n                                                  *,\\n                                                  year(data_corte) ano_corte,\\n                                                  month(data_corte) mes_corte,\\n                                                  count(distinct data_corte) over (partition by cdc_proc_lig, idclienteinquilino order by data_corte range between dateadd(current_date, - 1) and dateadd(current_date, -360)) as qtde_cortes12m_previos\\n                                                  from modelos_preditivos.kpi_maua_maua_processos_\\n                                                  where idobjetivoprocessocorte = 2\\n                                                  and not statusligacaoprocesso = &apos;C&apos;) t1\\n                                            )t2\\n                                      group by t2.ano_corte, t2.mes_corte\\n                                    ) t3\\n                                  inner join cortes_por_periodo t4\\n                                  on t3.ano_corte = t4.ano_corte\\n                                  and t3.mes_corte = t4.mes_corte\\n                                \\n&quot;</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cortados_12m_previos  = get_cortados_naocortados12mesesprev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (unidade_operacional == \"17\"):\n",
    "  if (cidade == \"43\"):\n",
    "    df_cortados_12m_previos.write.saveAsTable(name= \"modelos_preditivos.kpi_saneatins_palmas_cortados12M_\", mode = 'overwrite')\n",
    "  elif (cidade != \"43\"):\n",
    "    df_cortados_12m_previos.write.saveAsTable(name= \"modelos_preditivos.kpi_saneatins_all_cortados12M_\", mode = 'overwrite')\n",
    "elif (unidade_operacional == \"3\"):\n",
    "  if (cidade == \"3\"):\n",
    "    df_cortados_12m_previos.write.saveAsTable(name= \"modelos_preditivos.kpi_maua_maua_cortados12M_\", mode = 'overwrite')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  },
  "name": "cortados_nao_cortados_12m_previos (rascunho em desuso)",
  "notebookId": 2748700984678769
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
